name: Upload Cleaned CSVs to GCS

# üß† PURPOSE:
# This GitHub Actions workflow performs the entire ETL pipeline for the Real Estate Bubble Detection project:
# 1. Downloads raw economic indicator data from FRED using API scripts
# 2. Cleans and transforms the raw CSVs using your data cleaning script
# 3. Uploads the cleaned files to Google Cloud Storage (GCS)
# ‚úÖ Ensures all steps are automated without needing to commit data files to GitHub

on:
  # üîò Manually trigger the workflow from the GitHub Actions UI
  workflow_dispatch:

  # ‚è∞ Scheduled trigger every Monday at 8 AM EST (12 PM UTC)
  schedule:
    - cron: '0 12 * * 1'

jobs:
  upload:
    runs-on: ubuntu-latest  # üñ•Ô∏è Run this job on the latest Ubuntu virtual machine

    steps:
    # -----------------------------------------
    # üßæ STEP 1: Clone the repository onto the runner
    # -----------------------------------------
    - name: Checkout repository
      uses: actions/checkout@v3

    # -----------------------------------------
    # üêç STEP 2: Set up Python environment
    # This ensures compatibility with your scripts
    # -----------------------------------------
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    # -----------------------------------------
    # üì¶ STEP 3: Install Python dependencies
    # Uses pip to install everything from requirements.txt
    # -----------------------------------------
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # -----------------------------------------
    # üîê STEP 4: Decode base64 GCP credentials into a JSON file
    # Avoids JSONDecodeError from improperly escaped newlines
    # -----------------------------------------
    - name: Write GCP credentials from base64 secret
      run: |
        echo "${{ secrets.GCP_CREDENTIALS_JSON }}" | base64 --decode > gcp_creds.json

    # -----------------------------------------
    # üìÇ STEP 5: Create local folders for raw and cleaned data
    # Prevents "Folder not found" errors during execution
    # -----------------------------------------
    - name: Create folders for raw and clean data
      run: |
        mkdir -p data/raw
        mkdir -p data/clean

    # -----------------------------------------
    # üêõ STEP 6: Debug - Print partial FRED API key
    # Helps confirm the secret is loading properly (without exposing it)
    # -----------------------------------------
    - name: Debug - Echo partial FRED_API_KEY
      env:
        FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
      run: |
        echo "FRED_API_KEY first 5 chars: ${FRED_API_KEY:0:5}"
        echo "FRED_API_KEY length: ${#FRED_API_KEY}"

    # -----------------------------------------
    # üß≤ STEP 7: Download raw FRED data from API
    # Executes scripts to:
    # - Pull ZIPs from FRED for housing + macro indicators
    # - Extract and save raw .csvs into data/raw/
    # -----------------------------------------
    - name: Run FRED data ingestion scripts
      env:
        FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
      run: |
        python scripts/download_housing_starts_fred_data.py       # üè† Housing starts/permits/completions
        python scripts/fetching_additional_indicators.py          # üìâ CPI, UNRATE, FEDFUNDS, etc.

    # -----------------------------------------
    # üßπ STEP 8: Run data cleaning logic
    # This script reads from data/raw/ and writes cleaned .csvs to data/clean/
    # -----------------------------------------
    - name: Run data cleaning script
      run: python scripts/clean_fred_data.py

    # -----------------------------------------
    # ‚òÅÔ∏è STEP 9: Upload cleaned CSVs to Google Cloud Storage
    # Uses your Python uploader script to move data/clean/*.csv to GCS
    # -----------------------------------------
    - name: Run uploader script to GCS
      run: python scripts/upload_cleaned_data_to_gcs.py

    # ------------------------------------------------
    # üö® STEP 10: Log failure details (runs only if workflow fails)
    # Helps you capture logs or trigger alerting integrations later
    # ------------------------------------------------
    - name: üö® Log failure summary (if anything fails)
      if: failure()
      run: |
        echo "‚ùå Workflow failed! Check logs above ‚òùÔ∏è"
        echo "Consider re-running manually or inspecting the failing script."
        echo "You can later add Slack/email alert integrations here."

    # -----------------------------------------
    # üßº STEP 11: Cleanup temp files (optional)
    # You can uncomment this to delete raw data and credentials after job finishes
    # -----------------------------------------
    # - name: Cleanup
    #   run: rm -rf data/ gcp_creds.json